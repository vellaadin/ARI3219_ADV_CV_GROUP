{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# YoloV5 Object Detection Model - Adin Vella"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/ultralytics/yolov5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gitpython>=3.1.30 in /Users/adinvella/anaconda3/lib/python3.11/site-packages (from -r yolov5/requirements.txt (line 5)) (3.1.43)\n",
      "Requirement already satisfied: matplotlib>=3.3 in /Users/adinvella/anaconda3/lib/python3.11/site-packages (from -r yolov5/requirements.txt (line 6)) (3.7.2)\n",
      "Requirement already satisfied: numpy>=1.23.5 in /Users/adinvella/anaconda3/lib/python3.11/site-packages (from -r yolov5/requirements.txt (line 7)) (1.24.4)\n",
      "Requirement already satisfied: opencv-python>=4.1.1 in /Users/adinvella/anaconda3/lib/python3.11/site-packages (from -r yolov5/requirements.txt (line 8)) (4.10.0.84)\n",
      "Requirement already satisfied: pillow>=10.3.0 in /Users/adinvella/anaconda3/lib/python3.11/site-packages (from -r yolov5/requirements.txt (line 9)) (11.0.0)\n",
      "Requirement already satisfied: psutil in /Users/adinvella/anaconda3/lib/python3.11/site-packages (from -r yolov5/requirements.txt (line 10)) (5.9.0)\n",
      "Requirement already satisfied: PyYAML>=5.3.1 in /Users/adinvella/anaconda3/lib/python3.11/site-packages (from -r yolov5/requirements.txt (line 11)) (6.0)\n",
      "Requirement already satisfied: requests>=2.32.2 in /Users/adinvella/anaconda3/lib/python3.11/site-packages (from -r yolov5/requirements.txt (line 12)) (2.32.3)\n",
      "Requirement already satisfied: scipy>=1.4.1 in /Users/adinvella/anaconda3/lib/python3.11/site-packages (from -r yolov5/requirements.txt (line 13)) (1.11.1)\n",
      "Requirement already satisfied: thop>=0.1.1 in /Users/adinvella/anaconda3/lib/python3.11/site-packages (from -r yolov5/requirements.txt (line 14)) (0.1.1.post2209072238)\n",
      "Requirement already satisfied: torch>=1.8.0 in /Users/adinvella/anaconda3/lib/python3.11/site-packages (from -r yolov5/requirements.txt (line 15)) (2.3.0)\r\n",
      "Requirement already satisfied: torchvision>=0.9.0 in /Users/adinvella/anaconda3/lib/python3.11/site-packages (from -r yolov5/requirements.txt (line 16)) (0.18.0)\r\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /Users/adinvella/anaconda3/lib/python3.11/site-packages (from -r yolov5/requirements.txt (line 17)) (4.67.1)\r\n",
      "Requirement already satisfied: ultralytics>=8.2.34 in /Users/adinvella/anaconda3/lib/python3.11/site-packages (from -r yolov5/requirements.txt (line 18)) (8.3.56)\r\n",
      "Requirement already satisfied: pandas>=1.1.4 in /Users/adinvella/anaconda3/lib/python3.11/site-packages (from -r yolov5/requirements.txt (line 27)) (2.2.3)\r\n",
      "Requirement already satisfied: seaborn>=0.11.0 in /Users/adinvella/anaconda3/lib/python3.11/site-packages (from -r yolov5/requirements.txt (line 28)) (0.12.2)\r\n",
      "Requirement already satisfied: setuptools>=70.0.0 in /Users/adinvella/anaconda3/lib/python3.11/site-packages (from -r yolov5/requirements.txt (line 42)) (75.6.0)\r\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /Users/adinvella/anaconda3/lib/python3.11/site-packages (from gitpython>=3.1.30->-r yolov5/requirements.txt (line 5)) (4.0.11)\r\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Users/adinvella/anaconda3/lib/python3.11/site-packages (from matplotlib>=3.3->-r yolov5/requirements.txt (line 6)) (1.0.5)\r\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/adinvella/anaconda3/lib/python3.11/site-packages (from matplotlib>=3.3->-r yolov5/requirements.txt (line 6)) (0.11.0)\r\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/adinvella/anaconda3/lib/python3.11/site-packages (from matplotlib>=3.3->-r yolov5/requirements.txt (line 6)) (4.25.0)\r\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /Users/adinvella/anaconda3/lib/python3.11/site-packages (from matplotlib>=3.3->-r yolov5/requirements.txt (line 6)) (1.4.4)\r\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/adinvella/anaconda3/lib/python3.11/site-packages (from matplotlib>=3.3->-r yolov5/requirements.txt (line 6)) (23.1)\r\n",
      "Requirement already satisfied: pyparsing<3.1,>=2.3.1 in /Users/adinvella/anaconda3/lib/python3.11/site-packages (from matplotlib>=3.3->-r yolov5/requirements.txt (line 6)) (3.0.9)\r\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/adinvella/anaconda3/lib/python3.11/site-packages (from matplotlib>=3.3->-r yolov5/requirements.txt (line 6)) (2.8.2)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/adinvella/anaconda3/lib/python3.11/site-packages (from requests>=2.32.2->-r yolov5/requirements.txt (line 12)) (2.0.4)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/adinvella/anaconda3/lib/python3.11/site-packages (from requests>=2.32.2->-r yolov5/requirements.txt (line 12)) (3.4)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/adinvella/anaconda3/lib/python3.11/site-packages (from requests>=2.32.2->-r yolov5/requirements.txt (line 12)) (1.26.16)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/adinvella/anaconda3/lib/python3.11/site-packages (from requests>=2.32.2->-r yolov5/requirements.txt (line 12)) (2024.12.14)\r\n",
      "Requirement already satisfied: filelock in /Users/adinvella/anaconda3/lib/python3.11/site-packages (from torch>=1.8.0->-r yolov5/requirements.txt (line 15)) (3.9.0)\r\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /Users/adinvella/anaconda3/lib/python3.11/site-packages (from torch>=1.8.0->-r yolov5/requirements.txt (line 15)) (4.11.0)\r\n",
      "Requirement already satisfied: sympy in /Users/adinvella/anaconda3/lib/python3.11/site-packages (from torch>=1.8.0->-r yolov5/requirements.txt (line 15)) (1.11.1)\r\n",
      "Requirement already satisfied: networkx in /Users/adinvella/anaconda3/lib/python3.11/site-packages (from torch>=1.8.0->-r yolov5/requirements.txt (line 15)) (3.1)\r\n",
      "Requirement already satisfied: jinja2 in /Users/adinvella/anaconda3/lib/python3.11/site-packages (from torch>=1.8.0->-r yolov5/requirements.txt (line 15)) (3.1.2)\r\n",
      "Requirement already satisfied: fsspec in /Users/adinvella/anaconda3/lib/python3.11/site-packages (from torch>=1.8.0->-r yolov5/requirements.txt (line 15)) (2024.12.0)\r\n",
      "Requirement already satisfied: py-cpuinfo in /Users/adinvella/anaconda3/lib/python3.11/site-packages (from ultralytics>=8.2.34->-r yolov5/requirements.txt (line 18)) (8.0.0)\r\n",
      "Requirement already satisfied: ultralytics-thop>=2.0.0 in /Users/adinvella/anaconda3/lib/python3.11/site-packages (from ultralytics>=8.2.34->-r yolov5/requirements.txt (line 18)) (2.0.13)\r\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/adinvella/anaconda3/lib/python3.11/site-packages (from pandas>=1.1.4->-r yolov5/requirements.txt (line 27)) (2023.3.post1)\r\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/adinvella/anaconda3/lib/python3.11/site-packages (from pandas>=1.1.4->-r yolov5/requirements.txt (line 27)) (2023.3)\r\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /Users/adinvella/anaconda3/lib/python3.11/site-packages (from gitdb<5,>=4.0.1->gitpython>=3.1.30->-r yolov5/requirements.txt (line 5)) (5.0.1)\r\n",
      "Requirement already satisfied: six>=1.5 in /Users/adinvella/anaconda3/lib/python3.11/site-packages (from python-dateutil>=2.7->matplotlib>=3.3->-r yolov5/requirements.txt (line 6)) (1.16.0)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/adinvella/anaconda3/lib/python3.11/site-packages (from jinja2->torch>=1.8.0->-r yolov5/requirements.txt (line 15)) (2.1.1)\r\n",
      "Requirement already satisfied: mpmath>=0.19 in /Users/adinvella/anaconda3/lib/python3.11/site-packages (from sympy->torch>=1.8.0->-r yolov5/requirements.txt (line 15)) (1.3.0)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install -r yolov5/requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from yolov5 import train, val, detect\n",
    "import torch\n",
    "import os\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_yaml = \"/Users/adinvella/Documents/Artificial Intelligence/YR3/SEM1/ARI3129 - Advanced Computer Vision for Artificial Intelligence/Assignments/Group/ARI3219_ADV_CV_GROUP/Individual_Datasets/OnlineDataset1/garbage/data.yaml\"\n",
    "output_dir = \"/Users/adinvella/Documents/Artificial Intelligence/YR3/SEM1/ARI3129 - Advanced Computer Vision for Artificial Intelligence/Assignments/Group/ARI3219_ADV_CV_GROUP/Models/yolov5_pytorch/runs/train\"\n",
    "\n",
    "train_params = {\n",
    "    'data': dataset_yaml,\n",
    "    'cfg': \"yolov5/models/yolov5s.yaml\",\n",
    "    'weights': \"yolov5s.pt\",\n",
    "    'img_size': 640,\n",
    "    'batch_size': 16,\n",
    "    'epochs': 50,\n",
    "    'device': 0 if torch.cuda.is_available() else 'cpu',\n",
    "    'project': output_dir,\n",
    "    'name': 'waste_detection'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mweights=yolov5s.pt, cfg=yolov5/models/yolov5s.yaml, data=/Users/adinvella/Documents/Artificial Intelligence/YR3/SEM1/ARI3129 - Advanced Computer Vision for Artificial Intelligence/Assignments/Group/ARI3219_ADV_CV_GROUP/Individual_Datasets/OnlineDataset1/garbage/data.yaml, hyp=yolov5/data/hyps/hyp.scratch-low.yaml, epochs=50, batch_size=16, imgsz=640, rect=False, resume=False, nosave=False, noval=False, noautoanchor=False, noplots=False, evolve=None, evolve_population=yolov5/data/hyps, resume_evolve=None, bucket=, cache=None, image_weights=False, device=cpu, multi_scale=False, single_cls=False, optimizer=SGD, sync_bn=False, workers=8, project=/Users/adinvella/Documents/Artificial Intelligence/YR3/SEM1/ARI3129 - Advanced Computer Vision for Artificial Intelligence/Assignments/Group/ARI3219_ADV_CV_GROUP/Models/yolov5_pytorch/runs/train, name=waste_detection, exist_ok=False, quad=False, cos_lr=False, label_smoothing=0.0, patience=100, freeze=[0], save_period=-1, seed=0, local_rank=-1, entity=None, upload_dataset=False, bbox_interval=-1, artifact_alias=latest, ndjson_console=False, ndjson_file=False, img_size=640\n",
      "\u001b[34m\u001b[1mgithub: \u001b[0mup to date with https://github.com/ultralytics/yolov5 ✅\n",
      "fatal: cannot change to '/Users/adinvella/Documents/Artificial': No such file or directory\n",
      "YOLOv5 🚀 2025-1-1 Python-3.11.5 torch-2.3.0 CPU\n",
      "\n",
      "\u001b[34m\u001b[1mhyperparameters: \u001b[0mlr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=0.05, cls=0.5, cls_pw=1.0, obj=1.0, obj_pw=1.0, iou_t=0.2, anchor_t=4.0, fl_gamma=0.0, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0\n",
      "\u001b[34m\u001b[1mComet: \u001b[0mrun 'pip install comet_ml' to automatically track and visualize YOLOv5 🚀 runs in Comet\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir /Users/adinvella/Documents/Artificial Intelligence/YR3/SEM1/ARI3129 - Advanced Computer Vision for Artificial Intelligence/Assignments/Group/ARI3219_ADV_CV_GROUP/Models/yolov5_pytorch/runs/train', view at http://localhost:6006/\n",
      "Downloading https://github.com/ultralytics/assets/releases/download/v0.0.0/Arial.ttf to /Users/adinvella/Library/Application Support/Ultralytics/Arial.ttf...\n",
      "\r  0%|          | 0.00/755k [00:00<?, ?B/s]\r100%|██████████| 755k/755k [00:00<00:00, 7.43MB/s]\r100%|██████████| 755k/755k [00:00<00:00, 7.38MB/s]\n",
      "Downloading https://github.com/ultralytics/yolov5/releases/download/v7.0/yolov5s.pt to yolov5s.pt...\n",
      "\r  0%|          | 0.00/14.1M [00:00<?, ?B/s]\r  3%|▎         | 384k/14.1M [00:00<00:04, 3.27MB/s]\r 13%|█▎        | 1.88M/14.1M [00:00<00:01, 9.76MB/s]\r 20%|██        | 2.88M/14.1M [00:00<00:01, 9.10MB/s]\r 40%|███▉      | 5.62M/14.1M [00:00<00:00, 14.0MB/s]\r 66%|██████▋   | 9.38M/14.1M [00:00<00:00, 21.6MB/s]\r 86%|████████▌ | 12.1M/14.1M [00:00<00:00, 23.7MB/s]\r100%|██████████| 14.1M/14.1M [00:00<00:00, 19.1MB/s]\n",
      "\n",
      "Overriding model.yaml nc=80 with nc=2\n",
      "\n",
      "                 from  n    params  module                                  arguments                     \n",
      "  0                -1  1      3520  models.common.Conv                      [3, 32, 6, 2, 2]              \n",
      "  1                -1  1     18560  models.common.Conv                      [32, 64, 3, 2]                \n",
      "  2                -1  1     18816  models.common.C3                        [64, 64, 1]                   \n",
      "  3                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               \n",
      "  4                -1  2    115712  models.common.C3                        [128, 128, 2]                 \n",
      "  5                -1  1    295424  models.common.Conv                      [128, 256, 3, 2]              \n",
      "  6                -1  3    625152  models.common.C3                        [256, 256, 3]                 \n",
      "  7                -1  1   1180672  models.common.Conv                      [256, 512, 3, 2]              \n",
      "  8                -1  1   1182720  models.common.C3                        [512, 512, 1]                 \n",
      "  9                -1  1    656896  models.common.SPPF                      [512, 512, 5]                 \n",
      " 10                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n",
      " 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
      " 12           [-1, 6]  1         0  models.common.Concat                    [1]                           \n",
      " 13                -1  1    361984  models.common.C3                        [512, 256, 1, False]          \n",
      " 14                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n",
      " 15                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
      " 16           [-1, 4]  1         0  models.common.Concat                    [1]                           \n",
      " 17                -1  1     90880  models.common.C3                        [256, 128, 1, False]          \n",
      " 18                -1  1    147712  models.common.Conv                      [128, 128, 3, 2]              \n",
      " 19          [-1, 14]  1         0  models.common.Concat                    [1]                           \n",
      " 20                -1  1    296448  models.common.C3                        [256, 256, 1, False]          \n",
      " 21                -1  1    590336  models.common.Conv                      [256, 256, 3, 2]              \n",
      " 22          [-1, 10]  1         0  models.common.Concat                    [1]                           \n",
      " 23                -1  1   1182720  models.common.C3                        [512, 512, 1, False]          \n",
      " 24      [17, 20, 23]  1     18879  models.yolo.Detect                      [2, [[10, 13, 16, 30, 33, 23], [30, 61, 62, 45, 59, 119], [116, 90, 156, 198, 373, 326]], [128, 256, 512]]\n",
      "YOLOv5s summary: 214 layers, 7025023 parameters, 7025023 gradients, 16.0 GFLOPs\n",
      "\n",
      "Transferred 342/349 items from yolov5s.pt\n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m SGD(lr=0.01) with parameter groups 57 weight(decay=0.0), 60 weight(decay=0.0005), 60 bias\n",
      "\r\u001b[34m\u001b[1mtrain: \u001b[0mScanning /Users/adinvella/Documents/Artificial Intelligence/YR3/SEM1/ARI3129 - Advanced Computer Vision for Artificial Intelligence/Assignments/Group/ARI3219_ADV_CV_GROUP/Individual_Datasets/OnlineDataset1/garbage/train/labels...:   0%|          | 0/3310 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error decoding JSON from /Users/adinvella/Library/Application Support/Ultralytics/settings.json. Starting with an empty dictionary.\n",
      "Creating new Ultralytics Settings v0.0.6 file ✅ \n",
      "View Ultralytics Settings with 'yolo settings' or at '/Users/adinvella/Library/Application Support/Ultralytics/settings.json'\n",
      "Update Settings with 'yolo settings key=value', i.e. 'yolo settings runs_dir=path/to/dir'. For help see https://docs.ultralytics.com/quickstart/#ultralytics-settings.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /Users/adinvella/Documents/Artificial Intelligence/YR3/SEM1/ARI3129 - Advanced Computer Vision for Artificial Intelligence/Assignments/Group/ARI3219_ADV_CV_GROUP/Individual_Datasets/OnlineDataset1/garbage/train/labels... 1 images, 1 backgrounds, 0 corrupt:   0%|          | 1/3310 [00:06<6:11:26,  6.74s/it]\r\u001b[34m\u001b[1mtrain: \u001b[0mScanning /Users/adinvella/Documents/Artificial Intelligence/YR3/SEM1/ARI3129 - Advanced Computer Vision for Artificial Intelligence/Assignments/Group/ARI3219_ADV_CV_GROUP/Individual_Datasets/OnlineDataset1/garbage/train/labels... 651 images, 53 backgrounds, 0 corrupt:  20%|█▉        | 651/3310 [00:06<00:19, 135.14it/s]\r\u001b[34m\u001b[1mtrain: \u001b[0mScanning /Users/adinvella/Documents/Artificial Intelligence/YR3/SEM1/ARI3129 - Advanced Computer Vision for Artificial Intelligence/Assignments/Group/ARI3219_ADV_CV_GROUP/Individual_Datasets/OnlineDataset1/garbage/train/labels... 1301 images, 67 backgrounds, 0 corrupt:  39%|███▉      | 1301/3310 [00:06<00:06, 318.53it/s]\r\u001b[34m\u001b[1mtrain: \u001b[0mScanning /Users/adinvella/Documents/Artificial Intelligence/YR3/SEM1/ARI3129 - Advanced Computer Vision for Artificial Intelligence/Assignments/Group/ARI3219_ADV_CV_GROUP/Individual_Datasets/OnlineDataset1/garbage/train/labels... 2015 images, 83 backgrounds, 0 corrupt:  61%|██████    | 2015/3310 [00:07<00:02, 588.20it/s]\r\u001b[34m\u001b[1mtrain: \u001b[0mScanning /Users/adinvella/Documents/Artificial Intelligence/YR3/SEM1/ARI3129 - Advanced Computer Vision for Artificial Intelligence/Assignments/Group/ARI3219_ADV_CV_GROUP/Individual_Datasets/OnlineDataset1/garbage/train/labels... 2727 images, 87 backgrounds, 0 corrupt:  82%|████████▏ | 2727/3310 [00:07<00:00, 936.97it/s]\r\u001b[34m\u001b[1mtrain: \u001b[0mScanning /Users/adinvella/Documents/Artificial Intelligence/YR3/SEM1/ARI3129 - Advanced Computer Vision for Artificial Intelligence/Assignments/Group/ARI3219_ADV_CV_GROUP/Individual_Datasets/OnlineDataset1/garbage/train/labels... 3310 images, 215 backgrounds, 0 corrupt: 100%|██████████| 3310/3310 [00:07<00:00, 458.22it/s]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /Users/adinvella/Documents/Artificial Intelligence/YR3/SEM1/ARI3129 - Advanced Computer Vision for Artificial Intelligence/Assignments/Group/ARI3219_ADV_CV_GROUP/Individual_Datasets/OnlineDataset1/garbage/train/labels.cache\n",
      "\r\u001b[34m\u001b[1mval: \u001b[0mScanning /Users/adinvella/Documents/Artificial Intelligence/YR3/SEM1/ARI3129 - Advanced Computer Vision for Artificial Intelligence/Assignments/Group/ARI3219_ADV_CV_GROUP/Individual_Datasets/OnlineDataset1/garbage/valid/labels...:   0%|          | 0/193 [00:00<?, ?it/s]\r\u001b[34m\u001b[1mval: \u001b[0mScanning /Users/adinvella/Documents/Artificial Intelligence/YR3/SEM1/ARI3129 - Advanced Computer Vision for Artificial Intelligence/Assignments/Group/ARI3219_ADV_CV_GROUP/Individual_Datasets/OnlineDataset1/garbage/valid/labels... 1 images, 1 backgrounds, 0 corrupt:   1%|          | 1/193 [00:08<28:28,  8.90s/it]\r\u001b[34m\u001b[1mval: \u001b[0mScanning /Users/adinvella/Documents/Artificial Intelligence/YR3/SEM1/ARI3129 - Advanced Computer Vision for Artificial Intelligence/Assignments/Group/ARI3219_ADV_CV_GROUP/Individual_Datasets/OnlineDataset1/garbage/valid/labels... 2 images, 1 backgrounds, 0 corrupt:   1%|          | 2/193 [00:09<11:57,  3.76s/it]\r\u001b[34m\u001b[1mval: \u001b[0mScanning /Users/adinvella/Documents/Artificial Intelligence/YR3/SEM1/ARI3129 - Advanced Computer Vision for Artificial Intelligence/Assignments/Group/ARI3219_ADV_CV_GROUP/Individual_Datasets/OnlineDataset1/garbage/valid/labels... 193 images, 10 backgrounds, 0 corrupt: 100%|██████████| 193/193 [00:09<00:00, 21.30it/s]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /Users/adinvella/Documents/Artificial Intelligence/YR3/SEM1/ARI3129 - Advanced Computer Vision for Artificial Intelligence/Assignments/Group/ARI3219_ADV_CV_GROUP/Individual_Datasets/OnlineDataset1/garbage/valid/labels.cache\n",
      "\n",
      "\u001b[34m\u001b[1mAutoAnchor: \u001b[0m5.85 anchors/target, 1.000 Best Possible Recall (BPR). Current anchors are a good fit to dataset ✅\n",
      "Plotting labels to /Users/adinvella/Documents/Artificial Intelligence/YR3/SEM1/ARI3129 - Advanced Computer Vision for Artificial Intelligence/Assignments/Group/ARI3219_ADV_CV_GROUP/Models/yolov5_pytorch/runs/train/waste_detection/labels.jpg... \n",
      "Plotting labels to /Users/adinvella/Documents/Artificial Intelligence/YR3/SEM1/ARI3129 - Advanced Computer Vision for Artificial Intelligence/Assignments/Group/ARI3219_ADV_CV_GROUP/Models/yolov5_pytorch/runs/train/waste_detection2/labels.jpg... \n",
      "Image sizes 640 train, 640 val\n",
      "Using 8 dataloader workers\n",
      "Logging results to \u001b[1m/Users/adinvella/Documents/Artificial Intelligence/YR3/SEM1/ARI3129 - Advanced Computer Vision for Artificial Intelligence/Assignments/Group/ARI3219_ADV_CV_GROUP/Models/yolov5_pytorch/runs/train/waste_detection2\u001b[0m\n",
      "Starting training for 50 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "\r  0%|          | 0/207 [00:00<?, ?it/s]\r       0/49         0G     0.1262    0.03016    0.03038         37        640:   0%|          | 0/207 [00:26<?, ?it/s]\r       0/49         0G     0.1262    0.03016    0.03038         37        640:   0%|          | 1/207 [00:32<1:51:13, 32.39s/it]\r       0/49         0G     0.1273    0.02948    0.03071         28        640:   0%|          | 1/207 [00:59<1:51:13, 32.39s/it]\r       0/49         0G     0.1273    0.02948    0.03071         28        640:   1%|          | 2/207 [00:59<1:39:35, 29.15s/it]\r       0/49         0G     0.1264    0.02902    0.03081         25        640:   1%|          | 2/207 [01:23<1:39:35, 29.15s/it]\r       0/49         0G     0.1264    0.02902    0.03081         25        640:   1%|▏         | 3/207 [01:23<1:32:02, 27.07s/it]\r       0/49         0G     0.1267    0.02894    0.03082         34        640:   1%|▏         | 3/207 [01:58<1:32:02, 27.07s/it]\r       0/49         0G     0.1267    0.02894    0.03082         34        640:   2%|▏         | 4/207 [01:58<1:42:05, 30.18s/it]\r       0/49         0G     0.1267    0.02894    0.03082         34        640:   2%|▏         | 4/207 [02:07<1:47:58, 31.92s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train\u001b[39m.\u001b[39mrun(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mtrain_params)\n",
      "File \u001b[0;32m~/Documents/Artificial Intelligence/YR3/SEM1/ARI3129 - Advanced Computer Vision for Artificial Intelligence/Assignments/Group/ARI3219_ADV_CV_GROUP/Models/yolov5_pytorch/yolov5/train.py:980\u001b[0m, in \u001b[0;36mrun\u001b[0;34m(**kwargs)\u001b[0m\n\u001b[1;32m    978\u001b[0m \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m kwargs\u001b[39m.\u001b[39mitems():\n\u001b[1;32m    979\u001b[0m     \u001b[39msetattr\u001b[39m(opt, k, v)\n\u001b[0;32m--> 980\u001b[0m main(opt)\n\u001b[1;32m    981\u001b[0m \u001b[39mreturn\u001b[39;00m opt\n",
      "File \u001b[0;32m~/Documents/Artificial Intelligence/YR3/SEM1/ARI3129 - Advanced Computer Vision for Artificial Intelligence/Assignments/Group/ARI3219_ADV_CV_GROUP/Models/yolov5_pytorch/yolov5/train.py:688\u001b[0m, in \u001b[0;36mmain\u001b[0;34m(opt, callbacks)\u001b[0m\n\u001b[1;32m    686\u001b[0m \u001b[39m# Train\u001b[39;00m\n\u001b[1;32m    687\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m opt\u001b[39m.\u001b[39mevolve:\n\u001b[0;32m--> 688\u001b[0m     train(opt\u001b[39m.\u001b[39mhyp, opt, device, callbacks)\n\u001b[1;32m    690\u001b[0m \u001b[39m# Evolve hyperparameters (optional)\u001b[39;00m\n\u001b[1;32m    691\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    692\u001b[0m     \u001b[39m# Hyperparameter evolution metadata (including this hyperparameter True-False, lower_limit, upper_limit)\u001b[39;00m\n\u001b[1;32m    693\u001b[0m     meta \u001b[39m=\u001b[39m {\n\u001b[1;32m    694\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mlr0\u001b[39m\u001b[39m\"\u001b[39m: (\u001b[39mFalse\u001b[39;00m, \u001b[39m1e-5\u001b[39m, \u001b[39m1e-1\u001b[39m),  \u001b[39m# initial learning rate (SGD=1E-2, Adam=1E-3)\u001b[39;00m\n\u001b[1;32m    695\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mlrf\u001b[39m\u001b[39m\"\u001b[39m: (\u001b[39mFalse\u001b[39;00m, \u001b[39m0.01\u001b[39m, \u001b[39m1.0\u001b[39m),  \u001b[39m# final OneCycleLR learning rate (lr0 * lrf)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    722\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mcopy_paste\u001b[39m\u001b[39m\"\u001b[39m: (\u001b[39mTrue\u001b[39;00m, \u001b[39m0.0\u001b[39m, \u001b[39m1.0\u001b[39m),  \u001b[39m# segment copy-paste (probability)\u001b[39;00m\n\u001b[1;32m    723\u001b[0m     }\n",
      "File \u001b[0;32m~/Documents/Artificial Intelligence/YR3/SEM1/ARI3129 - Advanced Computer Vision for Artificial Intelligence/Assignments/Group/ARI3219_ADV_CV_GROUP/Models/yolov5_pytorch/yolov5/train.py:413\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(hyp, opt, device, callbacks)\u001b[0m\n\u001b[1;32m    411\u001b[0m \u001b[39m# Forward\u001b[39;00m\n\u001b[1;32m    412\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mamp\u001b[39m.\u001b[39mautocast(amp):\n\u001b[0;32m--> 413\u001b[0m     pred \u001b[39m=\u001b[39m model(imgs)  \u001b[39m# forward\u001b[39;00m\n\u001b[1;32m    414\u001b[0m     loss, loss_items \u001b[39m=\u001b[39m compute_loss(pred, targets\u001b[39m.\u001b[39mto(device))  \u001b[39m# loss scaled by batch_size\u001b[39;00m\n\u001b[1;32m    415\u001b[0m     \u001b[39mif\u001b[39;00m RANK \u001b[39m!=\u001b[39m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m   1543\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/Artificial Intelligence/YR3/SEM1/ARI3129 - Advanced Computer Vision for Artificial Intelligence/Assignments/Group/ARI3219_ADV_CV_GROUP/Models/yolov5_pytorch/yolov5/models/yolo.py:270\u001b[0m, in \u001b[0;36mDetectionModel.forward\u001b[0;34m(self, x, augment, profile, visualize)\u001b[0m\n\u001b[1;32m    268\u001b[0m \u001b[39mif\u001b[39;00m augment:\n\u001b[1;32m    269\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_augment(x)  \u001b[39m# augmented inference, None\u001b[39;00m\n\u001b[0;32m--> 270\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_once(x, profile, visualize)\n",
      "File \u001b[0;32m~/Documents/Artificial Intelligence/YR3/SEM1/ARI3129 - Advanced Computer Vision for Artificial Intelligence/Assignments/Group/ARI3219_ADV_CV_GROUP/Models/yolov5_pytorch/yolov5/models/yolo.py:169\u001b[0m, in \u001b[0;36mBaseModel._forward_once\u001b[0;34m(self, x, profile, visualize)\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[39mif\u001b[39;00m profile:\n\u001b[1;32m    168\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_profile_one_layer(m, x, dt)\n\u001b[0;32m--> 169\u001b[0m x \u001b[39m=\u001b[39m m(x)  \u001b[39m# run\u001b[39;00m\n\u001b[1;32m    170\u001b[0m y\u001b[39m.\u001b[39mappend(x \u001b[39mif\u001b[39;00m m\u001b[39m.\u001b[39mi \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msave \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m)  \u001b[39m# save output\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \u001b[39mif\u001b[39;00m visualize:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m   1543\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/Artificial Intelligence/YR3/SEM1/ARI3129 - Advanced Computer Vision for Artificial Intelligence/Assignments/Group/ARI3219_ADV_CV_GROUP/Models/yolov5_pytorch/yolov5/models/common.py:340\u001b[0m, in \u001b[0;36mSPPF.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    338\u001b[0m y1 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mm(x)\n\u001b[1;32m    339\u001b[0m y2 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mm(y1)\n\u001b[0;32m--> 340\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcv2(torch\u001b[39m.\u001b[39mcat((x, y1, y2, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mm(y2)), \u001b[39m1\u001b[39m))\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m   1543\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/Artificial Intelligence/YR3/SEM1/ARI3129 - Advanced Computer Vision for Artificial Intelligence/Assignments/Group/ARI3219_ADV_CV_GROUP/Models/yolov5_pytorch/yolov5/models/common.py:87\u001b[0m, in \u001b[0;36mConv.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[1;32m     86\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Applies a convolution followed by batch normalization and an activation function to the input tensor `x`.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 87\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mact(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbn(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv(x)))\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m   1543\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/conv.py:460\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    459\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 460\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_conv_forward(\u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mweight, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbias)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/conv.py:456\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mzeros\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m    453\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mconv2d(F\u001b[39m.\u001b[39mpad(\u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode),\n\u001b[1;32m    454\u001b[0m                     weight, bias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride,\n\u001b[1;32m    455\u001b[0m                     _pair(\u001b[39m0\u001b[39m), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdilation, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroups)\n\u001b[0;32m--> 456\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mconv2d(\u001b[39minput\u001b[39m, weight, bias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride,\n\u001b[1;32m    457\u001b[0m                 \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdilation, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroups)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train.run(**train_params)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a12dd5db26b16ea2076a17f1c019856423f5b5f736e01dd0ef78cdbcde973eba"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
